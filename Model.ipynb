{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "e477e906-bb27-4283-95fb-045bf07cb9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Necessarry imports and data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "7b17fb6e-10b0-4dc4-90f5-6296161fd4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import nltk\n",
    "from datetime import datetime\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3119b2a-6fad-4db3-8d6f-e7c3fb525943",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "2b4c6d4a-927a-44a0-9a56-cf35767307ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing model for sentiment analysis\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "92e77dd5-254b-46d4-957b-7beca5d39519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare json data files.\n",
    "\n",
    "with open('Data/gdko-2023-round-1.json') as f:\n",
    "    gdko2023r1 = json.load(f)\n",
    "r2023r1 = 2182 # amount of votes in round.\n",
    "\n",
    "with open('Data/gdko-2023-round-2.json') as f:\n",
    "    gdko2023r2 = json.load(f)\n",
    "r2023r2 = 1003\n",
    "\n",
    "with open('Data/gdko-2023-round-3.json') as f:\n",
    "    gdko2023r3 = json.load(f)\n",
    "r2023r3 = 603\n",
    "\n",
    "with open('Data/gdko-2024-round-1.json') as f:\n",
    "    gdko2024r1 = json.load(f)\n",
    "r2024r1 = 1502\n",
    "\n",
    "with open('Data/gdko-2024-round-2.json') as f:\n",
    "    gdko2024r2 = json.load(f)\n",
    "r2024r2 = 715\n",
    "\n",
    "with open('Data/gdko-2024-round-3.json') as f:\n",
    "    gdko2024r3 = json.load(f)\n",
    "r2024r3 = 312\n",
    "\n",
    "with open('Data/gdko-2025-round-1.json') as f:\n",
    "    gdko2025r1 = json.load(f)\n",
    "r2025r1 = 4032\n",
    "\n",
    "with open('Data/gdko-2025-round-2.json') as f:\n",
    "    gdko2025r2 = json.load(f)\n",
    "r2025r2 = 1192\n",
    "\n",
    "with open('Data/gdko-2025-round-3.json') as f:\n",
    "    gdko2025r3 = json.load(f)\n",
    "r2025r3 = 412"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "be40e70f-bb92-45ea-9f52-9d756ca9470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model builder methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "2c8e87a4-04c4-46c8-a3ce-d6981883ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing. Creating training and prediction datasets with defined features (columns).\n",
    "\n",
    "def platforms_to_num(platforms):\n",
    "    platform_code = ''.join(['1' if platform in platforms else '0' for platform in ['web', 'windows', 'linux', 'osx']])\n",
    "    return int(hex(int(platform_code, 2))[2:].zfill(4), 16)\n",
    "    \n",
    "def data_preprocessing(data, precision,  ratings_max, is_training = True):\n",
    "    entries_list = [{'entry_id': entry_id, **entry_data} for entry_id, entry_data in data.items()]\n",
    "    df = pd.DataFrame(entries_list)\n",
    "    df = pd.concat([df, json_normalize(df['participant_info'])], axis=1)\n",
    "    df = df.drop(columns=['participant_info'])\n",
    "    df['rating_count_normalized'] = round(df['rating_count'] * 100.0 / ratings_max , 1)\n",
    "    df['vader_score_neg'] = 0\n",
    "    df['vader_score_neu'] = 0\n",
    "    df['vader_score_pos'] = 0\n",
    "    df['roberta_score_neg'] = 0\n",
    "    df['roberta_score_neu'] = 0\n",
    "    df['roberta_score_pos'] = 0\n",
    "    df['vader_score_compound'] = 0\n",
    "    df['platforms_numeric'] = df['platforms'].apply(platforms_to_num)\n",
    "    df['comments_received'] = df['comments_received'].replace('', '0')\n",
    "    if is_training:\n",
    "        df['overal_raw_rating'] = df['overal_raw_rating'].apply(lambda x: round(pd.to_numeric(x[1:], errors='coerce').mean(), precision) if len(x[1:]) > 0 else np.nan)\n",
    "        return df[\n",
    "            ['game_title',\n",
    "             'game_url', \n",
    "             'platforms', \n",
    "             'platforms_numeric',\n",
    "             'rating_count',\n",
    "             'rating_count_normalized',\n",
    "             'comments_received',\n",
    "             'participant',\n",
    "\n",
    "             'coolness',\n",
    "             \n",
    "             'profile_url',\n",
    "             \n",
    "             'comments_raw',\n",
    "             \n",
    "             'vader_score_neg',\n",
    "             'vader_score_neu',\n",
    "             'vader_score_pos',    \n",
    "             'vader_score_compound',\n",
    "             \n",
    "             'roberta_score_neg',\n",
    "             'roberta_score_neu',\n",
    "             'roberta_score_pos',\n",
    "             \n",
    "             'overal_raw_rating']\n",
    "        ]\n",
    "    else:\n",
    "        return df[\n",
    "            ['game_title',\n",
    "             'game_url', \n",
    "             'platforms', \n",
    "             'platforms_numeric',\n",
    "             'rating_count',\n",
    "             'rating_count_normalized',\n",
    "             'comments_received',\n",
    "             'participant',\n",
    "\n",
    "             'coolness',\n",
    "             \n",
    "             'profile_url',\n",
    "             \n",
    "             'comments_raw',\n",
    "             \n",
    "             'vader_score_neg',\n",
    "             'vader_score_neu',\n",
    "             'vader_score_pos',    \n",
    "             'vader_score_compound',\n",
    "             \n",
    "             'roberta_score_neg',\n",
    "             'roberta_score_neu',\n",
    "             'roberta_score_pos']\n",
    "        ]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f87f241-a8eb-4f53-9b27-da0fc1b69286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculations for sentiment analysis scores. Scores will be used as features for predicions.\n",
    "# Important factor is the max length in the tokenizer in roberta scores. it will consider sentences of that length, otherwise will trim or fill them.\n",
    "def vader_scores(data, precision):\n",
    "    for i in tqdm(range(0, len(data))):\n",
    "        compound = []\n",
    "        negative = []\n",
    "        positive = []\n",
    "        neutral = []\n",
    "        for row in data['comments_raw'][i]:\n",
    "            compound.append(sia.polarity_scores(row)['compound'])\n",
    "            negative.append(sia.polarity_scores(row)['neg'])\n",
    "            positive.append(sia.polarity_scores(row)['pos'])\n",
    "            neutral.append(sia.polarity_scores(row)['neu'])\n",
    "        data.loc[i,'vader_score_compound'] = round(100.0 * sum(compound) / len(compound), precision) if len(compound) > 0 else 0\n",
    "        data.loc[i,'vader_score_neg'] = round(100.0 * sum(negative) / len(negative), precision) if len(negative) > 0 else 0\n",
    "        data.loc[i,'vader_score_pos'] = round(100.0 * sum(positive) / len(positive), precision) if len(positive) > 0 else 0\n",
    "        data.loc[i,'vader_score_neu'] = round(100.0 * sum(neutral) / len(neutral), precision) if len(neutral) > 0 else 0\n",
    "\n",
    "def roberta_scores(data, precision):\n",
    "    for i in tqdm(range(0, len(data))):\n",
    "        negative = []\n",
    "        positive = []\n",
    "        neutral = []\n",
    "        for comment in data['comments_raw'][i]:\n",
    "            encoded_comment = tokenizer(comment, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "            output = model(**encoded_comment)\n",
    "            scores = output[0][0].detach().numpy()\n",
    "            scores = softmax(scores)\n",
    "            negative.append(scores[0])\n",
    "            neutral.append(scores[1])\n",
    "            positive.append(scores[2])\n",
    "        data.loc[i,'roberta_score_neg'] = round(100.0 * sum(negative) / len(negative), precision) if len(negative) > 0 else 0\n",
    "        data.loc[i,'roberta_score_pos'] = round(100.0 * sum(positive) / len(positive), precision) if len(positive) > 0 else 0\n",
    "        data.loc[i,'roberta_score_neu'] = round(100.0 * sum(neutral) / len(neutral), precision) if len(neutral) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "9320c447-fa02-493a-9c98-82aff176da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction and validation methods.\n",
    "\n",
    "def validate_model(training_dataset ,split):\n",
    "    dataset = training_dataset\n",
    "    features = dataset.drop('overal_raw_rating', axis=1)\n",
    "    target = dataset['overal_raw_rating']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=split)\n",
    "    rf_regressor = RandomForestRegressor()\n",
    "    rf_regressor.fit(X_train, y_train)\n",
    "    predictions = rf_regressor.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    #print(\"Mean Squared Error:\", mse)\n",
    "    return mse\n",
    "\n",
    "def predict(training_dataset, realtime_dataset):\n",
    "    features = training_dataset.drop('overal_raw_rating', axis=1)\n",
    "    target = training_dataset['overal_raw_rating']\n",
    "    \n",
    "    rf_regressor = RandomForestRegressor()\n",
    "    rf_regressor.fit(features, target)\n",
    "    \n",
    "    predicted_result = rf_regressor.predict(realtime_dataset)\n",
    "    print(predicted_result)\n",
    "    return predicted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "fd418b59-6a0b-4b54-bf41-72a6e62ce220",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using the model to predict based on loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "d293ba82-9229-4b5a-bd0f-1a324aa9ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = data_preprocessing(gdko2023r1, 1, r2023r1)\n",
    "d2 = data_preprocessing(gdko2023r2, 1, r2023r2)\n",
    "d3 = data_preprocessing(gdko2023r3, 1, r2023r3)\n",
    "d4 = data_preprocessing(gdko2024r1, 1, r2024r1)\n",
    "d5 = data_preprocessing(gdko2024r2, 1, r2024r2)\n",
    "d6 = data_preprocessing(gdko2024r3, 1, r2024r3)\n",
    "d7 = data_preprocessing(gdko2025r1, 1, r2025r1)\n",
    "d8 = data_preprocessing(gdko2025r2, 1, r2025r2)\n",
    "d9 = data_preprocessing(gdko2025r3, 1, r2025r3, False) # False for prediction dataset, otherwise will be used as training dataset.\n",
    "\n",
    "td = pd.concat([d1, d2, d3, d4, d5, d6, d7, d8], ignore_index=True) # Concatenated training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd22fac9-95fa-4601-9f10-02cbfbe2ea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(td.shape, d9.shape) # td - training, d9 - prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1a555f-36c0-4876-8773-07d7cbf32016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating roberta scores for each dataset\n",
    "roberta_scores(td, 1)\n",
    "roberta_scores(d9, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c330b5c-0a3d-4295-b311-36ef6508c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating vader scores\n",
    "vader_scores(td, 1)\n",
    "vader_scores(d9, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5dcd09-d90b-416e-9c1b-4f83d11cc114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_dataset(ds, is_training = True):\n",
    "    if is_training:\n",
    "        ds['overal_raw_rating'].fillna(0, inplace=True)\n",
    "        return ds[[\n",
    "            # 'rating_count_normalized',\n",
    "            'rating_count',\n",
    "            # 'comments_received',\n",
    "            # 'vader_score_neg',\n",
    "            # 'vader_score_neu',\n",
    "            'vader_score_pos',    \n",
    "            # 'vader_score_compound',\n",
    "            'platforms_numeric',\n",
    "            # 'roberta_score_neg',\n",
    "            # 'roberta_score_neu',\n",
    "            'roberta_score_pos',\n",
    "            # 'coolness',\n",
    "            'overal_raw_rating'\n",
    "        ]]\n",
    "    else:\n",
    "        return ds[[\n",
    "            # 'rating_count_normalized',\n",
    "            'rating_count',\n",
    "            # 'comments_received',\n",
    "            # 'vader_score_neg',\n",
    "            # 'vader_score_neu',\n",
    "            'vader_score_pos',    \n",
    "            # 'vader_score_compound',\n",
    "            'platforms_numeric',\n",
    "            # 'roberta_score_neg',\n",
    "            # 'roberta_score_neu',\n",
    "            'roberta_score_pos',\n",
    "            # 'coolness',\n",
    "        ]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6973695d-8962-4618-89a3-1cab4eb3d8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = prediction_dataset(td, True)\n",
    "training_dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d6ba5-9e8d-4974-bdc4-c28f28152a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "realtime_dataset = prediction_dataset(d9, False) \n",
    "realtime_dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd4e39a-49ff-4ff3-8427-6d2ac6a08abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_per_column = (training_dataset == '').sum()\n",
    "print(\"Number of empty cells per column:\")\n",
    "print(empty_per_column)\n",
    "\n",
    "missing_per_column = training_dataset.isna().sum()\n",
    "print(\"Number of missing values per column:\")\n",
    "print(missing_per_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3e4363-8046-416c-aa42-490ec46ede05",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = []\n",
    "for i in range (0,10):\n",
    "    mse.append(validate_model(training_dataset, 0.35))\n",
    "import statistics\n",
    "statistics.mean(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6114494a-651d-4102-89eb-be0c3421fa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(min(mse),max(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5bc54-6b96-4274-8307-9b6e7ed4f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predict(training_dataset, realtime_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "818bc482-f9d8-4e24-bf01-b0788e307572",
   "metadata": {},
   "outputs": [],
   "source": [
    "d9['results'] = results\n",
    "d9['results'] = round(d9['results'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8c8910-1ada-4a1b-91bf-d24d16d72ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "d9.to_csv('predictions_' + str(datetime.utcnow()) + '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
